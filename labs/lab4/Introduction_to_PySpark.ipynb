{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HnNnrLMrJLJ"
      },
      "source": [
        "## This notebook is part of the Apache Spark training delivered by CERN IT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuIyGgJQrJLK"
      },
      "source": [
        "Run this notebook from Jupyter with Python kernel\n",
        "- When running on CERN SWAN, do not attach the notebook to a Spark cluster, but rather run it locally on the SWAN container (which is the default)\n",
        "- If running this outside CERN SWAN, please make sure to have PySpark installed: `pip install pyspark`\n",
        "\n",
        "\n",
        "In order to run this notebook as slides:\n",
        " - on SWAN click on the button \"Enter/Exit RISE slideshow\" in the ribbon\n",
        " - on other environments please make sure to have the RISE extension installed `pip install RISE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyNKilPLrJLL"
      },
      "source": [
        "### SPARK DataFrame Hands-On Lab\n",
        "Contact: Luca.Canali@cern.ch\n",
        "\n",
        "### Objective: Perform Basic DataFrame Operations\n",
        "1. Creating DataFrames\n",
        "2. Select columns\n",
        "3. Add, rename and drop columns\n",
        "4. Filtering rows\n",
        "5. Aggregations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi1rnFFerJLM"
      },
      "source": [
        "## Getting started: create the SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPpy_C2GrNc8",
        "outputId": "08b21ca3-a4df-479b-f3b9-f00475e0e564"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install RISE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBj_o8nfKigu",
        "outputId": "553a70fc-1d95-41cb-b812-0986af09be8d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting RISE\n",
            "  Downloading rise-5.7.1-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: notebook>=6.0 in /usr/local/lib/python3.11/dist-packages (from RISE) (6.5.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (3.1.6)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (6.4.2)\n",
            "Requirement already satisfied: pyzmq<25,>=17 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (24.0.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (23.1.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (5.7.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (5.7.2)\n",
            "Requirement already satisfied: jupyter-client<8,>=5.3.4 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (6.1.12)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (0.2.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (7.16.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (1.6.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (6.17.1)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=6.0->RISE) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client<8,>=5.3.4->notebook>=6.0->RISE) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook>=6.0->RISE) (4.3.6)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=6.0->RISE) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=6.0->RISE) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=6.0->RISE) (2.18.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=6.0->RISE) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=6.0->RISE) (4.23.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.11/dist-packages (from terminado>=0.8.3->notebook>=6.0->RISE) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=6.0->RISE) (21.2.0)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel->notebook>=6.0->RISE) (1.8.0)\n",
            "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->notebook>=6.0->RISE) (7.34.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel->notebook>=6.0->RISE) (0.1.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel->notebook>=6.0->RISE) (5.9.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=6.0->RISE) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=6.0->RISE) (1.4.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->RISE) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.23.1->ipykernel->notebook>=6.0->RISE)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->RISE) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->RISE) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->RISE) (3.0.50)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->RISE) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.23.1->ipykernel->notebook>=6.0->RISE) (4.9.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.0->RISE) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.0->RISE) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.0->RISE) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=6.0->RISE) (0.23.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0->RISE) (1.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client<8,>=5.3.4->notebook>=6.0->RISE) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=6.0->RISE) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=6.0->RISE) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=6.0->RISE) (4.12.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=6.0->RISE) (2.22)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->notebook>=6.0->RISE) (0.8.4)\n",
            "Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0->RISE) (3.7.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0->RISE) (1.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->notebook>=6.0->RISE) (0.2.13)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0->RISE) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=6.0->RISE) (1.3.1)\n",
            "Downloading rise-5.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi, RISE\n",
            "Successfully installed RISE-5.7.1 jedi-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "hIg9iY8yrJLM",
        "outputId": "52a3b794-ee9c-4e45-ce6d-8a3fd84d27f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d231827acd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://dabb8348304d:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>DataFrame HandsOn 1</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "          .master(\"local[*]\") \\\n",
        "          .appName(\"DataFrame HandsOn 1\") \\\n",
        "          .config(\"spark.ui.showConsoleProgress\",\"false\") \\\n",
        "          .getOrCreate()\n",
        "        )\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikTo070grJLN"
      },
      "source": [
        "The master `local[*]` means that the executors are in the same node that is running the driver. The `*` tells Spark to start as many executors as there are logical cores available"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSJSGyY5rJLO"
      },
      "source": [
        "### Hands-On 1 - Construct a DataFrame from csv file\n",
        "This demostrates how to read a csv file and construct a DataFrame.  \n",
        "We will use the online retail dataset from Kaggle, credits: https://www.kaggle.com/datasets/vijayuv/onlineretail\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSjhlX-XrJLP"
      },
      "source": [
        "#### First, let's inspect the csv content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "152IyR1_rJLQ",
        "outputId": "673a1bc5-8324-4d4b-c010-7d817b3cc2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: ../data/online-retail-dataset.csv.gz: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "#modify below code to use the downloaded dataset\n",
        "\n",
        "!gzip -cd ../data/online-retail-dataset.csv.gz 2>&1| head -n3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8gtdhCvNf-A",
        "outputId": "6111a5fc-83c3-4786-9165-87f6a6513f27"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qybMDn5nrJLR"
      },
      "outputs": [],
      "source": [
        "online_retail_schema=\"InvoiceNo int, StockCode string, Description string, Quantity int,\\\n",
        "InvoiceDate timestamp,UnitPrice float,CustomerId int, Country string\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark.read\n",
        "      .option(\"header\", \"true\")\n",
        "      .option(\"timestampFormat\", \"M/d/yyyy H:m\")\n",
        "      .csv(\"/content/drive/MyDrive/Colab Notebooks/DE/OnlineRetail.csv\",\n",
        "           schema=online_retail_schema)\n",
        "     )\n"
      ],
      "metadata": {
        "id": "g9zIfPX6Q5EO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l13Fs5LHrJLR"
      },
      "source": [
        "#### Inspect the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true,
        "id": "M_QjfHXqrJLT",
        "outputId": "3632f1c3-5fcc-4485-af53-aa60765ffd5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerId|Country       |\n",
            "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
            "|536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850     |United Kingdom|\n",
            "|536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850     |United Kingdom|\n",
            "+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show(2, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wELzfkKrJLU"
      },
      "source": [
        "#### Show columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BKbNp-ABrJLU",
        "outputId": "a3c9008b-f8e7-413c-b06b-84f1e3d2f548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- InvoiceNo: integer (nullable = true)\n",
            " |-- StockCode: string (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- InvoiceDate: timestamp (nullable = true)\n",
            " |-- UnitPrice: float (nullable = true)\n",
            " |-- CustomerId: integer (nullable = true)\n",
            " |-- Country: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IG2FcvbrJLU"
      },
      "source": [
        "### Hands-On 2 - Spark Transformations - select, add, rename and drop columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWUZGPXHrJLU"
      },
      "source": [
        "Select dataframe columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": true,
        "id": "mtuDLKvtrJLU",
        "outputId": "f9e2d6b1-2689-49a1-8c5e-52442a602c68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|       Country|\n",
            "+--------------+\n",
            "|United Kingdom|\n",
            "|United Kingdom|\n",
            "+--------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# select single column\n",
        "\n",
        "df.select(\"Country\").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svYP-Nj6rJLU"
      },
      "source": [
        "Select multiple columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9PRdayYrrJLU",
        "outputId": "8140d664-f8d2-45b9-a774-e1ea3c716972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------------------------------+---------+\n",
            "|StockCode|Description                       |UnitPrice|\n",
            "+---------+----------------------------------+---------+\n",
            "|85123A   |WHITE HANGING HEART T-LIGHT HOLDER|2.55     |\n",
            "|71053    |WHITE METAL LANTERN               |3.39     |\n",
            "+---------+----------------------------------+---------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\"StockCode\",\"Description\",\"UnitPrice\").show(n=2, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gB4enTonrJLV",
        "outputId": "98f8f355-5ed2-4546-f568-bef747ef1475",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['InvoiceNo',\n",
              " 'StockCode',\n",
              " 'Description',\n",
              " 'Quantity',\n",
              " 'InvoiceDate',\n",
              " 'UnitPrice',\n",
              " 'CustomerId',\n",
              " 'Country']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9OuOr9j8rJLV",
        "outputId": "b8d9b18b-98f6-4000-a876-fa3a42958e8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|\n",
            "+---------+---------+--------------------+--------+-------------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|\n",
            "+---------+---------+--------------------+--------+-------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# select first 5 columns\n",
        "df.select(df.columns[0:5]).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OlY1qkVBrJLV",
        "outputId": "0900bdbc-33c5-411d-9abe-7b36f26256fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerId|       Country|HighValueItem|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------------+\n",
            "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|        false|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|        false|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# selects all the original columns and adds a new column that specifies high value item\n",
        "(df.selectExpr(\n",
        "   \"*\", # all original columns\n",
        "   \"(UnitPrice > 100) as HighValueItem\")\n",
        "   .show(2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0IiNqEgTrJLW",
        "outputId": "b385c8a1-4877-4c94-a45d-e01d7da9c293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------+\n",
            "|TotalQuantity|InventoryValue|\n",
            "+-------------+--------------+\n",
            "|      5176450|       2498803|\n",
            "+-------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# selects all the original columns and adds a new column that specifies high value item\n",
        "(df.selectExpr(\n",
        "  \"sum(Quantity) as TotalQuantity\",\n",
        "  \"cast(sum(UnitPrice) as int) as InventoryValue\")\n",
        "  .show()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvSNTynGrJLW"
      },
      "source": [
        "#### Adding, renaming and dropping columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "l1FQcKONrJLW",
        "outputId": "d37db5fd-b630-4c44-c75d-e89c7bad193f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------------------------------+---------+--------+------------+\n",
            "|InvoiceNo|Description                       |UnitPrice|Quantity|InvoiceValue|\n",
            "+---------+----------------------------------+---------+--------+------------+\n",
            "|536365   |WHITE HANGING HEART T-LIGHT HOLDER|2.55     |6       |15.299999   |\n",
            "|536365   |WHITE METAL LANTERN               |3.39     |6       |20.34       |\n",
            "+---------+----------------------------------+---------+--------+------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+---------+----------------------------------+---------+--------+---------+\n",
            "|InvoiceNo|Description                       |UnitPrice|Quantity|LineTotal|\n",
            "+---------+----------------------------------+---------+--------+---------+\n",
            "|536365   |WHITE HANGING HEART T-LIGHT HOLDER|2.55     |6       |15.299999|\n",
            "|536365   |WHITE METAL LANTERN               |3.39     |6       |20.34    |\n",
            "+---------+----------------------------------+---------+--------+---------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+---------+----------------------------------+---------+--------+\n",
            "|InvoiceNo|Description                       |UnitPrice|Quantity|\n",
            "+---------+----------------------------------+---------+--------+\n",
            "|536365   |WHITE HANGING HEART T-LIGHT HOLDER|2.55     |6       |\n",
            "|536365   |WHITE METAL LANTERN               |3.39     |6       |\n",
            "+---------+----------------------------------+---------+--------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# add a new column called InvoiceValue\n",
        "from pyspark.sql.functions import expr\n",
        "df_1 = (df\n",
        "        .withColumn(\"InvoiceValue\", expr(\"UnitPrice * Quantity\"))\n",
        "        .select(\"InvoiceNo\",\"Description\",\"UnitPrice\",\"Quantity\",\"InvoiceValue\")\n",
        "       )\n",
        "df_1.show(2, False)\n",
        "\n",
        "# rename InvoiceValue to LineTotal\n",
        "df_2 = df_1.withColumnRenamed(\"InvoiceValue\",\"LineTotal\")\n",
        "df_2.show(2, False)\n",
        "\n",
        "# drop a column\n",
        "df_2.drop(\"LineTotal\").show(2, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUOj7aBmrJLW"
      },
      "source": [
        "### Hands-On 3 - Spark Transformations - filter, sort and cast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lvKUpyYgrJLW",
        "outputId": "99798e81-6afa-43b6-8d1c-6f50912d2276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerId|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   556444|    22502|PICNIC BASKET WIC...|      60|2011-06-10 15:28:00|    649.5|     15098|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerId|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   556444|    22502|PICNIC BASKET WIC...|      60|2011-06-10 15:28:00|    649.5|     15098|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerId|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   556444|    22502|PICNIC BASKET WIC...|      60|2011-06-10 15:28:00|    649.5|     15098|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# select invoice lines with quantity > 50 and unitprice > 20\n",
        "df.where(col(\"Quantity\") > 20).where(col(\"UnitPrice\") > 50).show(2)\n",
        "df.filter(df.Quantity > 20).filter(df.UnitPrice > 50).show(2)\n",
        "df.filter(\"Quantity > 20 and UnitPrice > 50\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sjlaB_QDrJLW",
        "outputId": "17e61149-73e4-4ddd-eb8c-c4adb1a8670d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerId|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   536378|    21212|PACK OF 72 RETROS...|     120|2010-12-01 09:37:00|     0.42|     14688|United Kingdom|\n",
            "|     NULL|        D|            Discount|      -1|2010-12-01 09:41:00|     27.5|     14527|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# select invoice lines with quantity > 100 or unitprice > 20\n",
        "df.where((col(\"Quantity\") > 100) | (col(\"UnitPrice\") > 20)).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-hFuqWHVrJLX",
        "outputId": "c0939791-b76f-4f78-af96-7f84f3317431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+---------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|    Description|Quantity|        InvoiceDate|UnitPrice|CustomerId|       Country|\n",
            "+---------+---------+---------------+--------+-------------------+---------+----------+--------------+\n",
            "|     NULL|        B|Adjust bad debt|       1|2011-08-12 14:51:00|-11062.06|      NULL|United Kingdom|\n",
            "|     NULL|        B|Adjust bad debt|       1|2011-08-12 14:52:00|-11062.06|      NULL|United Kingdom|\n",
            "+---------+---------+---------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 2 rows\n",
            "\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerId|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   581483|    23843|PAPER CRAFT , LIT...|   80995|2011-12-09 09:15:00|     2.08|     16446|United Kingdom|\n",
            "|   541431|    23166|MEDIUM CERAMIC TO...|   74215|2011-01-18 10:01:00|     1.04|     12346|United Kingdom|\n",
            "|   578841|    84826|ASSTD DESIGN 3D P...|   12540|2011-11-25 15:57:00|      0.0|     13256|United Kingdom|\n",
            "|   542504|    37413|                NULL|    5568|2011-01-28 12:03:00|      0.0|      NULL|United Kingdom|\n",
            "|   573008|    84077|WORLD WAR 2 GLIDE...|    4800|2011-10-27 12:26:00|     0.21|     12901|United Kingdom|\n",
            "|   554868|    22197|SMALL POPCORN HOLDER|    4300|2011-05-27 10:52:00|     0.72|     13135|United Kingdom|\n",
            "|   556231|   85123A|                   ?|    4000|2011-06-09 15:04:00|      0.0|      NULL|United Kingdom|\n",
            "|   544612|    22053|EMPIRE DESIGN ROS...|    3906|2011-02-22 10:43:00|     0.82|     18087|United Kingdom|\n",
            "|   560599|    18007|ESSENTIAL BALM 3....|    3186|2011-07-19 17:04:00|     0.06|     14609|United Kingdom|\n",
            "|   540815|    21108|FAIRY CAKE FLANNE...|    3114|2011-01-11 12:55:00|      2.1|     15749|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import desc, asc\n",
        "\n",
        "# sort in the default order: ascending\n",
        "df.orderBy(expr(\"UnitPrice\")).show(2)\n",
        "\n",
        "df.orderBy(col(\"Quantity\").desc(), col(\"UnitPrice\").asc()).show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkxQOf0brJLX"
      },
      "source": [
        "### Hands-On 4 - Spark Transformations - aggregations\n",
        "full list of built int functions - https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "X0gIYcIprJLX",
        "outputId": "eeebd71d-f8ea-4472-a683-1ce5d3ebf0b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------+\n",
            "|count(DISTINCT CustomerID)|\n",
            "+--------------------------+\n",
            "|                      4372|\n",
            "+--------------------------+\n",
            "\n",
            "CPU times: user 14.8 ms, sys: 3.35 ms, total: 18.1 ms\n",
            "Wall time: 2.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Count distinct customers\n",
        "from pyspark.sql.functions import countDistinct\n",
        "df.select(countDistinct(\"CustomerID\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rIcYKGymrJLX",
        "outputId": "840581d9-7b64-47f7-9e3f-914e2e6ecebf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+\n",
            "|approx_count_distinct(CustomerID)|\n",
            "+---------------------------------+\n",
            "|                             4336|\n",
            "+---------------------------------+\n",
            "\n",
            "CPU times: user 8.89 ms, sys: 1.66 ms, total: 10.6 ms\n",
            "Wall time: 1.72 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# approx. distinct stock items\n",
        "from pyspark.sql.functions import approx_count_distinct\n",
        "df.select(approx_count_distinct(\"CustomerID\", 0.1)).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0FGqJXjgrJLX",
        "outputId": "f46099a9-11aa-4d84-c521-23bb957a8f6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+-------------+-------------+\n",
            "|   avg_purchases|max_purchases|min_purchases|\n",
            "+----------------+-------------+-------------+\n",
            "|9.55224954743324|        80995|       -80995|\n",
            "+----------------+-------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# average, maximum and minimum purchase quantity\n",
        "from pyspark.sql.functions import avg, max, min\n",
        "( df.select(\n",
        "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
        "    max(\"Quantity\").alias(\"max_purchases\"),\n",
        "    min(\"Quantity\").alias(\"min_purchases\"))\n",
        "   .show()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Wt3dqNrJLX"
      },
      "source": [
        "### Hands-On 5 - Spark Transformations - grouping and windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ivM-iICerJLY",
        "outputId": "04feed09-e305-41b2-9385-090efea34513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+----------+-----+\n",
            "|InvoiceNo|CustomerId|count|\n",
            "+---------+----------+-----+\n",
            "|   536573|     17025|    4|\n",
            "|   537228|     17677|    1|\n",
            "|   537419|     13495|   14|\n",
            "|   538093|     12682|   33|\n",
            "|   538648|     17937|    5|\n",
            "+---------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+---------+------------------+--------------------+\n",
            "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
            "+---------+------------------+--------------------+\n",
            "|   536532| 25.36986301369863|  16.850272831671976|\n",
            "|   537632|               1.0|                 0.0|\n",
            "|   538708| 10.61111111111111|   7.150282736359209|\n",
            "|   538877|14.258278145695364|   27.56989037543246|\n",
            "|   538993| 9.333333333333334|   2.748737083745107|\n",
            "+---------+------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# count of items on the invoice\n",
        "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)\n",
        "\n",
        "# grouping with expressions\n",
        "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n",
        "  .show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRr2f8chrJLY"
      },
      "source": [
        "### Read the csv file into DataFrame\n",
        "\n",
        "`%%time` is an iPython magic https://ipython.readthedocs.io/en/stable/interactive/magics.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yUqIPHrJLY"
      },
      "source": [
        "It's possible to read files without specifying the schema. Some file formats (Parquet is one of them) include the schema, which means that Spark can start reading the file. For format without schema (csv, json...) Spark can infer the schema. Let's see what's the difference in terms of time and of results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nqSkQm4RrJLY"
      },
      "outputs": [],
      "source": [
        "online_retail_schema=\"InvoiceNo int, StockCode string, Description string, Quantity int,\\\n",
        "InvoiceDate timestamp,UnitPrice float,CustomerId int, Country string\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sZLNaGJ-rJLl",
        "outputId": "f56e5f57-8a43-4c10-8432-22c18019b570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.12 ms, sys: 0 ns, total: 4.12 ms\n",
            "Wall time: 53.1 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"timestampFormat\", \"M/d/yyyy H:m\")\\\n",
        "        .csv(\"/content/drive/MyDrive/Colab Notebooks/DE/OnlineRetail.csv\",\n",
        "             schema=online_retail_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "scrolled": true,
        "id": "6KiCslK1rJLl",
        "outputId": "6cf56fa0-26a7-4eab-a084-7ebb49086e3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 34.4 ms, sys: 7 ms, total: 41.4 ms\n",
            "Wall time: 7.64 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "df_infer = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"inferSchema\", \"true\") \\\n",
        "        .csv(\"/content/drive/MyDrive/Colab Notebooks/DE/OnlineRetail.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BORYSMbTrJLl"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh8CsapArJLm"
      },
      "source": [
        "Reminder: documentation at\n",
        "https://spark.apache.org/docs/latest/api/python/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHuZbdsvrJLm"
      },
      "source": [
        "If you didn't run the previous cells, run the following one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OeDk8oMrJLm"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .appName(\"DataFrame HandsOn 1\") \\\n",
        "        .config(\"spark.ui.showConsoleProgress\",\"false\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "online_retail_schema=\"InvoiceNo int, StockCode string, Description string, Quantity int,\\\n",
        "InvoiceDate timestamp,UnitPrice float,CustomerId int, Country string\"\n",
        "\n",
        "df = spark.read \\\n",
        "        .option(\"header\", \"true\") \\\n",
        "        .option(\"timestampFormat\", \"M/d/yyyy H:m\")\\\n",
        "        .csv(\"../data/online-retail-dataset.csv.gz\",\n",
        "             schema=online_retail_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c33yzogYrJLm"
      },
      "source": [
        "Task: Show 5 lines of the \"description\" column"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"Description\").show(n=5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UovxhVST9Ra",
        "outputId": "1d2a4ab4-5ee5-41fa-ec4d-01db708edc2c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------+\n",
            "|Description                        |\n",
            "+-----------------------------------+\n",
            "|WHITE HANGING HEART T-LIGHT HOLDER |\n",
            "|WHITE METAL LANTERN                |\n",
            "|CREAM CUPID HEARTS COAT HANGER     |\n",
            "|KNITTED UNION FLAG HOT WATER BOTTLE|\n",
            "|RED WOOLLY HOTTIE WHITE HEART.     |\n",
            "+-----------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsnrYSYPrJLm"
      },
      "source": [
        "Task: Count the number of distinct invoices in the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Count distinct invoices\n",
        "from pyspark.sql.functions import countDistinct\n",
        "df.select(countDistinct(\"InvoiceNo\").alias(\"total_invoices\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFpEOabdT-oH",
        "outputId": "7247e2ee-56a2-4d6c-91a0-98f36be167e5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+\n",
            "|total_invoices|\n",
            "+--------------+\n",
            "|         22061|\n",
            "+--------------+\n",
            "\n",
            "CPU times: user 15.7 ms, sys: 4.77 ms, total: 20.5 ms\n",
            "Wall time: 2.99 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvm0QaeErJLm"
      },
      "source": [
        "Task: Find out in which month most invoices have been issued"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from pyspark.sql.functions import month\n",
        "\n",
        "df.withColumn(\"InvoiceMonth\", month(\"InvoiceDate\"))\\\n",
        "  .groupBy(\"InvoiceMonth\")\\\n",
        "  .count()\\\n",
        "  .orderBy(\"count\", ascending=False)\\\n",
        "  .show(1, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vwyg_KST_4g",
        "outputId": "5bda9bbb-0e14-4472-c195-0cdc74ee9dd0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "|InvoiceMonth|count|\n",
            "+------------+-----+\n",
            "|11          |84711|\n",
            "+------------+-----+\n",
            "only showing top 1 row\n",
            "\n",
            "CPU times: user 7.92 ms, sys: 3.48 ms, total: 11.4 ms\n",
            "Wall time: 2.14 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LEyMTWlrJLm"
      },
      "source": [
        "Task: Filter the lines where the Quantity is more than 30"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"Quantity\"] > 30).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5mS9y63UA3a",
        "outputId": "19c4a4bf-f3af-4bcd-8e2c-ee1e6ea77da3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerId|       Country|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|     13047|United Kingdom|\n",
            "|   536370|    10002|INFLATABLE POLITI...|      48|2010-12-01 08:45:00|     0.85|     12583|        France|\n",
            "|   536370|    22492|MINI PAINT SET VI...|      36|2010-12-01 08:45:00|     0.65|     12583|        France|\n",
            "|   536371|    22086|PAPER CHAIN KIT 5...|      80|2010-12-01 09:00:00|     2.55|     13748|United Kingdom|\n",
            "|   536374|    21258|VICTORIAN SEWING ...|      32|2010-12-01 09:09:00|    10.95|     15100|United Kingdom|\n",
            "|   536376|    22114|HOT WATER BOTTLE ...|      48|2010-12-01 09:32:00|     3.45|     15291|United Kingdom|\n",
            "|   536376|    21733|RED HANGING HEART...|      64|2010-12-01 09:32:00|     2.55|     15291|United Kingdom|\n",
            "|   536378|    21212|PACK OF 72 RETROS...|     120|2010-12-01 09:37:00|     0.42|     14688|United Kingdom|\n",
            "|   536378|   85183B|CHARLIE & LOLA WA...|      48|2010-12-01 09:37:00|     1.25|     14688|United Kingdom|\n",
            "|   536378|   85071B|RED CHARLIE+LOLA ...|      96|2010-12-01 09:37:00|     0.38|     14688|United Kingdom|\n",
            "|   536381|    22719|GUMBALL MONOCHROM...|      36|2010-12-01 09:41:00|     1.06|     15311|United Kingdom|\n",
            "|   536382|    22381|TOY TIDY PINK POL...|      50|2010-12-01 09:45:00|     1.85|     16098|United Kingdom|\n",
            "|   536384|    84755|COLOUR GLASS T-LI...|      48|2010-12-01 09:53:00|     0.65|     18074|United Kingdom|\n",
            "|   536384|    22469|HEART OF WICKER S...|      40|2010-12-01 09:53:00|     1.45|     18074|United Kingdom|\n",
            "|   536384|    22470|HEART OF WICKER L...|      40|2010-12-01 09:53:00|     2.55|     18074|United Kingdom|\n",
            "|   536386|    84880|WHITE WIRE EGG HO...|      36|2010-12-01 09:57:00|     4.95|     16029|United Kingdom|\n",
            "|   536386|   85099C|JUMBO  BAG BAROQU...|     100|2010-12-01 09:57:00|     1.65|     16029|United Kingdom|\n",
            "|   536386|   85099B|JUMBO BAG RED RET...|     100|2010-12-01 09:57:00|     1.65|     16029|United Kingdom|\n",
            "|   536387|    79321|       CHILLI LIGHTS|     192|2010-12-01 09:58:00|     3.82|     16029|United Kingdom|\n",
            "|   536387|    22780|LIGHT GARLAND BUT...|     192|2010-12-01 09:58:00|     3.37|     16029|United Kingdom|\n",
            "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVLCaW7PrJLm"
      },
      "source": [
        "Task: Show the four most sold items (by quantity)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum as _sum\n",
        "\n",
        "df.groupBy(\"Description\")\\\n",
        "  .agg(_sum(\"Quantity\").alias(\"TotalQuantity\"))\\\n",
        "  .orderBy(\"TotalQuantity\", ascending=False)\\\n",
        "  .show(4, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSWSqsSyUBoj",
        "outputId": "4aecd526-607c-407f-ce4e-386cc4a99a13"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------+-------------+\n",
            "|Description                      |TotalQuantity|\n",
            "+---------------------------------+-------------+\n",
            "|WORLD WAR 2 GLIDERS ASSTD DESIGNS|53847        |\n",
            "|JUMBO BAG RED RETROSPOT          |47363        |\n",
            "|ASSORTED COLOUR BIRD ORNAMENT    |36381        |\n",
            "|POPCORN HOLDER                   |36334        |\n",
            "+---------------------------------+-------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXgV1IYGrJLm"
      },
      "source": [
        "Bonus question: why do these two operations return different results? Hint: look at the documentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "UV-VeRZQrJLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aea0924-5c68-4073-b7d7-c098c5e4a425"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22062\n",
            "+-------------------------+\n",
            "|count(DISTINCT InvoiceNo)|\n",
            "+-------------------------+\n",
            "|                    22061|\n",
            "+-------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(df.select(\"InvoiceNo\").distinct().count())\n",
        "from pyspark.sql.functions import countDistinct\n",
        "df.select(countDistinct(\"InvoiceNo\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`df.select(\"InvoiceNo\").distinct().count()`\n",
        "This operation returns the count of all unique rows in the \"InvoiceNo\" column. If there's a null value, it will be considered as one distinct value.\n",
        "\n",
        "`df.select(countDistinct(\"InvoiceNo\")).show()`\n",
        "The countDistinct function ignores null values when counting distinct elements. Therefore, if there are any nulls in \"InvoiceNo\", they won’t be counted."
      ],
      "metadata": {
        "id": "dsuwlMH8WziT"
      }
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "sparkconnect": {
      "bundled_options": [],
      "list_of_options": []
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}