# -*- coding: utf-8 -*-
"""lab4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r6sPfu_q9R_3HL7iWpPDCaexMk30dF3H

## This notebook is part of the Apache Spark training delivered by CERN IT

Run this notebook from Jupyter with Python kernel
- When running on CERN SWAN, do not attach the notebook to a Spark cluster, but rather run it locally on the SWAN container (which is the default)
- If running this outside CERN SWAN, please make sure to have PySpark installed: `pip install pyspark`


In order to run this notebook as slides:
 - on SWAN click on the button "Enter/Exit RISE slideshow" in the ribbon
 - on other environments please make sure to have the RISE extension installed `pip install RISE`

### SPARK DataFrame Hands-On Lab
Contact: Luca.Canali@cern.ch

### Objective: Perform Basic DataFrame Operations
1. Creating DataFrames
2. Select columns
3. Add, rename and drop columns
4. Filtering rows
5. Aggregations

## Getting started: create the SparkSession
"""

!pip install pyspark

# !pip install pyspark

from pyspark.sql import SparkSession

spark = (SparkSession.builder
          .master("local[*]") \
          .appName("DataFrame HandsOn 1") \
          .config("spark.ui.showConsoleProgress","false") \
          .getOrCreate()
        )

spark

"""The master `local[*]` means that the executors are in the same node that is running the driver. The `*` tells Spark to start as many executors as there are logical cores available

### Hands-On 1 - Construct a DataFrame from csv file
This demostrates how to read a csv file and construct a DataFrame.  
We will use the online retail dataset from Kaggle, credits: https://www.kaggle.com/datasets/vijayuv/onlineretail

#### First, let's inspect the csv content
"""

#modify below code to use the downloaded dataset

!gzip -cd ../data/online-retail-dataset.csv.gz 2>&1| head -n3

!pip install kaggle

!kaggle datasets download -d vijayuv/onlineretail
!unzip onlineretail.zip

!mv /content/OnlineRetail.csv /content/online_retail_dataset.csv

online_retail_schema="InvoiceNo int, StockCode string, Description string, Quantity int,\
InvoiceDate timestamp,UnitPrice float,CustomerId int, Country string"

!ls /content

df = spark.read.csv("online_retail_dataset.csv", header=True, schema=online_retail_schema)
df.show(5)

"""#### Inspect the data"""

df.show(2, False)

"""#### Show columns"""

df.printSchema()

"""### Hands-On 2 - Spark Transformations - select, add, rename and drop columns

Select dataframe columns
"""

# select single column

df.select("Country").show(2)

"""Select multiple columns

"""

df.select("StockCode","Description","UnitPrice").show(n=2, truncate=False)

df.columns

# select first 5 columns
df.select(df.columns[0:5]).show(2)

# selects all the original columns and adds a new column that specifies high value item
(df.selectExpr(
   "*", # all original columns
   "(UnitPrice > 100) as HighValueItem")
   .show(2)
)

# selects all the original columns and adds a new column that specifies high value item
(df.selectExpr(
  "sum(Quantity) as TotalQuantity",
  "cast(sum(UnitPrice) as int) as InventoryValue")
  .show()
)

"""#### Adding, renaming and dropping columns"""

# add a new column called InvoiceValue
from pyspark.sql.functions import expr
df_1 = (df
        .withColumn("InvoiceValue", expr("UnitPrice * Quantity"))
        .select("InvoiceNo","Description","UnitPrice","Quantity","InvoiceValue")
       )
df_1.show(2, False)

# rename InvoiceValue to LineTotal
df_2 = df_1.withColumnRenamed("InvoiceValue","LineTotal")
df_2.show(2, False)

# drop a column
df_2.drop("LineTotal").show(2, False)

"""### Hands-On 3 - Spark Transformations - filter, sort and cast"""

from pyspark.sql.functions import col

# select invoice lines with quantity > 50 and unitprice > 20
df.where(col("Quantity") > 20).where(col("UnitPrice") > 50).show(2)
df.filter(df.Quantity > 20).filter(df.UnitPrice > 50).show(2)
df.filter("Quantity > 20 and UnitPrice > 50").show(2)

# select invoice lines with quantity > 100 or unitprice > 20
df.where((col("Quantity") > 100) | (col("UnitPrice") > 20)).show(2)

from pyspark.sql.functions import desc, asc

# sort in the default order: ascending
df.orderBy(expr("UnitPrice")).show(2)

df.orderBy(col("Quantity").desc(), col("UnitPrice").asc()).show(10)

"""### Hands-On 4 - Spark Transformations - aggregations
full list of built int functions - https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Count distinct customers
# from pyspark.sql.functions import countDistinct
# df.select(countDistinct("CustomerID")).show()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # approx. distinct stock items
# from pyspark.sql.functions import approx_count_distinct
# df.select(approx_count_distinct("CustomerID", 0.1)).show()

# average, maximum and minimum purchase quantity
from pyspark.sql.functions import avg, max, min
( df.select(
    avg("Quantity").alias("avg_purchases"),
    max("Quantity").alias("max_purchases"),
    min("Quantity").alias("min_purchases"))
   .show()
)

"""### Hands-On 5 - Spark Transformations - grouping and windows"""

# count of items on the invoice
df.groupBy("InvoiceNo", "CustomerId").count().show(5)

# grouping with expressions
df.groupBy("InvoiceNo").agg(expr("avg(Quantity)"),expr("stddev_pop(Quantity)"))\
  .show(5)

"""### Read the csv file into DataFrame

`%%time` is an iPython magic https://ipython.readthedocs.io/en/stable/interactive/magics.html

It's possible to read files without specifying the schema. Some file formats (Parquet is one of them) include the schema, which means that Spark can start reading the file. For format without schema (csv, json...) Spark can infer the schema. Let's see what's the difference in terms of time and of results:
"""

online_retail_schema="InvoiceNo int, StockCode string, Description string, Quantity int,\
InvoiceDate timestamp,UnitPrice float,CustomerId int, Country string"

# Commented out IPython magic to ensure Python compatibility.
# %%time
# df = spark.read \
# .option("header", "true") \
# .option("timestampFormat", "M/d/yyyy H:m").csv("online_retail_dataset.csv", schema=online_retail_schema)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# df_infer = spark.read \
# .option("header", "true") \
# .option("inferSchema", "true") \
# .csv("online_retail_dataset.csv")

"""So, it appears from experimentation that the inference of schema takes more time compared to when we provide the schema explicitly.

## Exercises

Reminder: documentation at
https://spark.apache.org/docs/latest/api/python/index.html

If you didn't run the previous cells, run the following one:
"""

from pyspark.sql import SparkSession

spark = SparkSession.builder \
        .master("local[*]") \
        .appName("DataFrame HandsOn 1") \
        .config("spark.ui.showConsoleProgress","false") \
        .getOrCreate()

online_retail_schema="InvoiceNo int, StockCode string, Description string, Quantity int,\
InvoiceDate timestamp,UnitPrice float,CustomerId int, Country string"

df = spark.read \
        .option("header", "true") \
        .option("timestampFormat", "M/d/yyyy H:m")\
        .csv("../data/online-retail-dataset.csv.gz",
             schema=online_retail_schema)

"""Task: Show 5 lines of the "description" column"""

df.select("Description").show(n=5,truncate=False)

"""Task: Count the number of distinct invoices in the dataframe"""

from pyspark.sql.functions import countDistinct
df.select(countDistinct("InvoiceNo")).show()

"""Task: Find out in which month most invoices have been issued"""

from pyspark.sql.functions import month, count, desc

df_month = df.withColumn("Month", month("InvoiceDate"))
month_counts = df_month.groupBy("Month").agg(count("InvoiceNo").alias("InvoiceCount")).orderBy(desc("InvoiceCount")).show()

"""Task: Filter the lines where the Quantity is more than 30"""

df.where((col("Quantity") > 30)).show(n=5)

df.filter(df.Quantity > 30).count()

"""Task: Show the four most sold items (by quantity)"""

from pyspark.sql.functions import sum

df_sum_quantity = df.groupBy("Description").agg(sum("Quantity").alias("TotalQuantity"))
df_sum_quantity.orderBy(desc("TotalQuantity")).show(4)

"""Bonus question: why do these two operations return different results? Hint: look at the documentation"""

print(df.select("InvoiceNo").distinct().count())
from pyspark.sql.functions import countDistinct
df.select(countDistinct("InvoiceNo")).show()

df_notnull = df.filter(df.InvoiceNo.isNotNull()) # Checking if null values values make a difference
print(df_notnull.select("InvoiceNo").distinct().count())

"""## Why these two operations return two different results?

Answer: Importing SQL function means that the NULL values would not be considered as a distinct value, thus the counts differ by 1, in both the operations. I knew this earlier from MySQL.
"""